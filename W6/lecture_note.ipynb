{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Modeling Connections Between Neurons\n",
    "\n",
    "In this lecture, we are guided through a journey into computational neuroscience, focusing particularly on synapses and neural networks. Here's a breakdown of key points:\n",
    "\n",
    "1. **Neuroscience Review** (Week 1):\n",
    "   - We learned about neurons, synapses, and brain regions.\n",
    "   - Explored **neural encoding models**, spike-triggered averages, and the **Poisson model** for spiking neurons.\n",
    "   \n",
    "2. **Neural Decoding** (Subsequent Weeks):\n",
    "   - Focused on discriminating and decoding stimuli using neural activities.\n",
    "   - Introduced **Information Theory** and its role in neural coding.\n",
    "\n",
    "3. **Mechanistic Models** (Prior Week):\n",
    "   - Studied single neuron models like the **RC Circuit model** and the **Hodgkin-Huxley model**, which explains action potential generation.\n",
    "   - Learned about simplified neuron models, such as the **Integrate and Fire model**.\n",
    "\n",
    "4. **Synaptic Networks** (Current Week):\n",
    "   - Introduction to **chemical synapses**, their role in connecting neurons, and modeling their impact on membrane potentials.\n",
    "   - Explored excitatory synapses (glutamate leading to depolarization) and inhibitory synapses (GABA causing hyperpolarization).\n",
    "   - Modeled the synaptic impact on membrane potential using the **RC Circuit model** and expanded it to include synaptic conductance.\n",
    "\n",
    "5. **Modeling Synaptic Conductance**:\n",
    "   - The synapse is modeled using **synaptic conductance** (Gs) and the synaptic **equilibrium potential** (Vs).\n",
    "   - Synaptic conductance changes based on input spikes and is modeled using an exponential function for excitatory synapses (like **AMPA synapse**) or an **alpha function** for other synapses (like **GABA-A** or **NMDA synapse**).\n",
    "\n",
    "6. **Network Models**:\n",
    "   - We were introduced to modeling networks of connected neurons, where synaptic conductance and spike trains are combined using linear filters to simulate the collective behavior of neurons in a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Introduction to Network Models\n",
    "\n",
    "#### **1. Spiking Neurons vs. Firing Rate Neurons**\n",
    "- **Spiking Neurons:**\n",
    "  - **Advantages:** Allows modeling of computation and learning based on **spike timing**. Useful for synchrony modeling.\n",
    "  - **Disadvantages:** Computationally expensive due to the need for simulating differential equations, challenging for large networks.\n",
    "  \n",
    "- **Firing Rate Neurons:**\n",
    "  - **Advantages:** Efficient for modeling large networks, outputs are real-valued firing rates.\n",
    "  - **Disadvantages:** Cannot model phenomena based on spike timing or synchrony.\n",
    "\n",
    "#### **2. Relationship Between Spiking and Firing Rate Models**\n",
    "- **Linear Filter Model:** Synaptic conductance at a neuron can be modeled by filtering spike trains (ρ) through a synaptic filter (K(t)).\n",
    "  - **Total Synaptic Input:** A summation of weighted inputs from multiple synapses is used.\n",
    "  - The spiking model can be converted into a firing rate model unless input neurons are correlated or synchronized.\n",
    "  \n",
    "- **Simplification of Equations:**\n",
    "  - Using an **exponential filter** allows simplification by removing integrals.\n",
    "  - The **input current (Is)** changes with time similar to an RC circuit. Time constant (τ_s) controls reaction speed.\n",
    "  \n",
    "#### **3. Firing Rate Networks**\n",
    "- **Single Output Neuron:** The neuron’s firing rate (V) evolves over time based on synaptic inputs (W × U) with a time constant (τ_r).\n",
    "  - If **τ_s < τ_r**, the synaptic input converges quickly: V depends on W × U.\n",
    "  - If **τ_r < τ_s**, V follows a nonlinear function of the synaptic current.\n",
    "  - In the **steady-state** (for static inputs), the output firing rate is a function of the input synaptic current (W × U), similar to artificial neural network models.\n",
    "\n",
    "- **Multiple Output Neurons:** Extends the model to a vector of outputs with synaptic weights in matrix form (W). This leads to a system of differential equations where output firing rates are determined by W × U.\n",
    "\n",
    "#### **4. Recurrent Networks**\n",
    "- **Recurrent Feedback Connections:** Neurons can connect to each other within a layer, forming feedback loops. The recurrent connections are described by a feedback weight matrix (M), added to the existing feedforward input (W × U).\n",
    "  - If M = 0, the network behaves as a **feedforward network**.\n",
    "  \n",
    "#### **5. Linear Feedforward Networks**\n",
    "- **Example:** A simple feedforward network can be used to perform **linear filtering**, detecting edges in input data.\n",
    "  - **Matrix W:** Contains shifted versions of a filter that detects changes in input values.\n",
    "  - The network implements **linear transformations** to detect sudden changes, such as edges in an image.\n",
    "\n",
    "#### **6. Edge Detection and the Brain**\n",
    "- The brain performs **edge detection** using receptive fields in the primary visual cortex (V1). These fields resemble filters that compute changes in image brightness.\n",
    "  - **First-order Derivative:** The linear filter approximates the **first derivative**, detecting transitions (edges).\n",
    "  - **Second-order Derivative:** Other receptive fields perform a second-order derivative, detecting bars and other features.\n",
    "\n",
    "#### **Key Takeaways:**\n",
    "- Spiking models are precise but computationally costly, while firing rate models are efficient but lack temporal precision.\n",
    "- **Linear filtering** operations performed by networks in AI are approximations of the **calculus** the brain performs to detect changes in sensory input (like edges in vision).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 The Fascinating World of Recurrent Networks\n",
    "\n",
    "It sounds like you're diving deep into the intricacies of recurrent networks! Let’s break it down and highlight some of the key points from the video:\n",
    "\n",
    "1. **Linear Recurrent Networks**: \n",
    "   - These networks involve neurons where the output $ v(t) $ depends on both the input $ W \\times U $ and recurrent connections $ M \\times v(t) $. \n",
    "   - Symmetry in the recurrent connection matrix $ M $ ensures that eigenvectors and eigenvalues play a critical role in determining the network's behavior.\n",
    "   - Eigenvectors help break down the problem by expressing $ v(t) $ in terms of a linear combination of orthonormal eigenvectors. These eigenvectors form a new coordinate system to simplify the understanding of $ v(t) $.\n",
    "\n",
    "2. **Eigenvalue Dynamics**:\n",
    "   - The eigenvalues $ \\lambda_i $ of $ M $ control the stability of the network:\n",
    "     - If $ \\lambda_i > 1 $, the output grows exponentially, leading to instability.\n",
    "     - If all $ \\lambda_i < 1 $, the network converges to a steady state, implying stability.\n",
    "     - If one $ \\lambda_i = 1 $, the network maintains a memory of past inputs (integrating the input over time).\n",
    "\n",
    "3. **Amplification**: \n",
    "   - The network can amplify its inputs, particularly if one eigenvalue is close to 1. In this case, the output gets amplified as shown by the equation $ \\frac{1}{1 - \\lambda_1} $, where $ \\lambda_1 $ is close to 1.\n",
    "\n",
    "4. **Nonlinear Recurrent Networks**:\n",
    "   - When introducing non-linearity (e.g., rectification non-linearity that ensures neuron firing rates don't go below 0), the network can still amplify inputs but now suppress irrelevant peaks.\n",
    "   - This allows the network to exhibit **selective attention** (focusing on one part of the input while ignoring others) and **gain modulation** (multiplying the effect of an input shift in a multiplicative manner on the output).\n",
    "\n",
    "5. **Non-Symmetric Recurrent Networks**:\n",
    "   - These involve excitatory and inhibitory neurons where the connections are no longer symmetric. These networks have more complex dynamics.\n",
    "   - **Jacobian (or Stability) Matrix**: In analyzing the stability of such networks, eigenvalues of the Jacobian matrix, which may be complex (having real and imaginary parts), determine whether the network will exhibit oscillatory behavior or settle into a stable state.\n",
    "\n",
    "Recurrent networks, both linear and non-linear, demonstrate how feedback loops in the system can lead to powerful behaviors like memory retention, amplification, and filtering of signals—mirroring some of the capabilities of biological neural systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
