{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Neurons as CLassifiers and Supervised Learning\n",
    "\n",
    "### **Supervised Learning and Perceptrons**\n",
    "\n",
    "1. **Classification Problem**:\n",
    "   - **Task**: Given images, classify them as containing faces or not.\n",
    "   - **Approach**: Use machine learning to find a hyperplane that separates the images into classes.\n",
    "\n",
    "2. **Perceptron Model**:\n",
    "   - **Basic Idea**: A perceptron sums its inputs, compares the sum to a threshold, and outputs a result based on this comparison.\n",
    "   - **Mathematical Model**: If the weighted sum of inputs exceeds the threshold, the output is +1; otherwise, it’s -1.\n",
    "   - **Geometric Interpretation**: The perceptron essentially defines a hyperplane (or a line in 2D) that separates classes.\n",
    "\n",
    "3. **Learning in Perceptrons**:\n",
    "   - **Objective**: Adjust weights and threshold to correctly classify inputs.\n",
    "   - **Learning Rule**: Update weights based on the error between desired and actual outputs.\n",
    "   - **Limitations**: Perceptrons can only classify linearly separable data. For more complex tasks, multilayer perceptrons (MLPs) are needed.\n",
    "\n",
    "4. **Multilayer Perceptrons (MLPs)**:\n",
    "   - **Structure**: Consist of multiple layers (input, hidden, and output layers).\n",
    "   - **Function**: Can handle more complex, non-linear classification problems like XOR.\n",
    "   - **Continuous Outputs**: Use functions like the sigmoid function for regression tasks, providing outputs in a continuous range.\n",
    "\n",
    "5. **Training MLPs**:\n",
    "   - **Backpropagation**: An algorithm used to minimize error by propagating errors backward through the network and adjusting weights accordingly.\n",
    "   - **Gradient Descent**: Used to find the optimal weights by minimizing the error function.\n",
    "\n",
    "6. **Application Example**:\n",
    "   - **Truck Backing Simulation**: Demonstrates using MLPs for practical tasks like backing a truck into a loading dock, showing the training process and gradual improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Reinforcement Learning: Predicting Rewards\n",
    "\n",
    "\n",
    "1. **Reinforcement Learning (RL):** In RL, an agent (like Pavlov's dog) interacts with an environment and aims to maximize the cumulative future rewards. The agent learns by exploring actions and receiving feedback in the form of rewards or punishments.\n",
    "\n",
    "2. **Pavlov’s Classical Conditioning:** Pavlov’s experiments with dogs showed that animals can learn to predict rewards based on stimuli (like a bell). This prediction can lead to anticipatory responses, such as salivation.\n",
    "\n",
    "3. **Temporal Difference (TD) Learning:** To predict future rewards, RL algorithms use TD learning, which updates predictions based on differences between successive predictions. Richard Bellman’s dynamic programming approach helps in recursively calculating these updates, handling the challenge of missing future rewards.\n",
    "\n",
    "4. **Neuroscientific Insights:** Studies have shown that dopamine neurons in the ventral tegmental area (VTA) of the brain encode reward prediction errors. Before learning, these neurons respond strongly at the time of reward. After learning, they respond more to the predictive stimulus and less to the reward itself. This aligns with TD learning principles.\n",
    "\n",
    "5. **Prediction Errors:** If a reward is omitted when expected, it results in a negative prediction error, leading to a decrease in neuron firing rates in the VTA. This reflects the discrepancy between expected and actual outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Reinforcement Learning: Time for Action!\n",
    "\n",
    "1. **Reinforcement Learning Framework**:\n",
    "   - The framework involves an agent interacting with an environment, where the agent observes the state, receives rewards, and takes actions. \n",
    "   - The goal is to learn a policy that maximizes the expected total future reward.\n",
    "\n",
    "2. **Policy and Value**:\n",
    "   - A policy ($\\pi$) maps states to actions to maximize expected rewards.\n",
    "   - The value of a state (or the expected reward for being in that state) is crucial for deciding which action to take.\n",
    "\n",
    "3. **Example with the Rat**:\n",
    "   - In a maze with varying food rewards, the rat needs to decide whether to go left or right at each location to maximize future rewards.\n",
    "   - States are the locations in the maze, and actions are \"go left\" or \"go right.\"\n",
    "   - The expected reward for each state is computed based on the rewards of neighboring states.\n",
    "\n",
    "4. **Temporal Difference Learning (TD Learning)**:\n",
    "   - TD Learning is used to update the value of states based on the difference between predicted and actual rewards.\n",
    "   - It helps the agent learn the value of each state as it explores the environment.\n",
    "\n",
    "5. **Actor-Critic Learning**:\n",
    "   - This algorithm has two components:\n",
    "     - **Critic**: Evaluates the current policy by learning the value of states using TD Learning.\n",
    "     - **Actor**: Selects actions based on a probabilistic policy that uses the Q-function (value of state-action pairs) and the softmax function to balance exploration and exploitation.\n",
    "   - The algorithm learns an optimal policy by repeating the process of evaluating and improving the policy.\n",
    "\n",
    "6. **Exploration vs. Exploitation**:\n",
    "   - Initially, the agent explores various actions to learn about the environment.\n",
    "   - Over time, it exploits the knowledge to choose actions that maximize rewards based on learned values.\n",
    "\n",
    "7. **Application to Real-World Problems**:\n",
    "   - Reinforcement learning has been applied to complex tasks like autonomous helicopter flight, showcasing its practical utility.\n",
    "\n",
    "8. **Mapping to the Brain**:\n",
    "   - There are theoretical parallels between reinforcement learning components and brain structures like the basal ganglia.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
