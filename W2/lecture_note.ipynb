{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 What is the Neural Code?\n",
    "\n",
    "**Overview**\n",
    "- The focus of this week is neural coding: how information is represented and transformed in the brain.\n",
    "- The study of neural coding involves understanding how sensory information is converted into neural activity and how this activity can be measured and analyzed.\n",
    "\n",
    "**Techniques for Recording Brain Activity**\n",
    "1. **Functional Magnetic Resonance Imaging (fMRI)**\n",
    "   - Measures blood oxygenation changes in the brain.\n",
    "   - Provides spatial information on brain activity but with slow temporal resolution.\n",
    "   - Useful for identifying active regions during tasks.\n",
    "\n",
    "2. **Electroencephalography (EEG)**\n",
    "   - Captures electrical changes in the brainâ€™s electric fields via electrodes on the scalp.\n",
    "   - Faster than fMRI but noisy and less spatially precise.\n",
    "   - Useful for studying dynamic brain processes.\n",
    "\n",
    "3. **Multi-Electrode Arrays**\n",
    "   - Records activity from many neurons simultaneously.\n",
    "   - Used in brain slices for high-resolution data.\n",
    "   - Example: Array developed by Alan Litke with 512 electrodes.\n",
    "\n",
    "4. **Penetrating Electrodes**\n",
    "   - Used to record from neurons in vivo, allowing observation of neuronal activity during behavior.\n",
    "   - Can be moved to target different neurons or depths.\n",
    "\n",
    "5. **Calcium Imaging**\n",
    "   - Measures fluorescence changes in neurons due to calcium binding, indicating neuronal firing.\n",
    "   - Can record from thousands of neurons at once.\n",
    "\n",
    "6. **Patch Electrodes**\n",
    "   - Directly contact the inside of a neuron to measure electrical activity.\n",
    "\n",
    "**Neural Code and Representation**\n",
    "- **Neural Encoding**\n",
    "  - Examines how stimuli (like visual or auditory signals) cause specific patterns of neural responses.\n",
    "  - Models predict neural responses to stimuli based on measured activity.\n",
    "\n",
    "- **Neural Decoding**\n",
    "  - Determines the stimulus based on recorded neural responses.\n",
    "  - Often involves probabilistic models to account for noise and variability.\n",
    "\n",
    "**Experimental Data Examples**\n",
    "1. **Retinal Ganglion Cells**\n",
    "   - Response to visual stimuli (e.g., movies) is analyzed through raster plots showing action potentials over time.\n",
    "   - Different cells respond to different features of the stimuli.\n",
    "\n",
    "2. **Tuning Curves**\n",
    "   - **Visual Cortex (V1)**: Neurons have tuning curves for stimulus orientation, often Gaussian-shaped.\n",
    "   - **Motor Cortex**: Neurons show tuning curves related to movement direction, often cosine-shaped.\n",
    "\n",
    "3. **Functional Maps**\n",
    "   - Neurons in visual cortex have orderly maps of feature sensitivity (e.g., orientation, spatial frequency).\n",
    "   - Examples include pinwheel structures and orientation maps in V1.\n",
    "\n",
    "4. **Complex Stimulus Representations**\n",
    "   - Neurons may respond to complex or semantic stimuli, such as specific faces or concepts.\n",
    "   - Example: Neurons in the parahippocampal area respond to images of celebrities or their names.\n",
    "\n",
    "**Hierarchical Representation**\n",
    "- **Progression of Features**\n",
    "  - From simple sensory representations (e.g., edges in V1) to complex semantic categories (e.g., faces in higher brain areas).\n",
    "  - Higher-order areas integrate and abstract features, influencing initial sensory processing through feedback.\n",
    "\n",
    "**Top-Down Effects**\n",
    "- **Influence of Expectations**\n",
    "  - Semantic context and expectations can shape sensory perception.\n",
    "  - Example: Recognition of ambiguous images improves with familiar context.\n",
    "\n",
    "**Conclusion**\n",
    "- The study of neural coding involves both understanding the mechanisms of information representation and developing models to predict and interpret neural responses. The integration of various recording techniques and the hierarchical nature of feature processing provide a comprehensive view of how the brain encodes and decodes information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Neural Encoding Simple Models\n",
    "\n",
    "### Academic Summary: Constructing Neural Response Models\n",
    "\n",
    "**Overview**\n",
    "- This section discusses how to model neural responses to stimuli, focusing on how the firing rate $r(t)$ of a neuron depends on an input stimulus $s(t)$. \n",
    "\n",
    "**Basic Model: Linear Dependence**\n",
    "1. **Simple Linear Relationship**\n",
    "   - The simplest model assumes that the neuron's response $r(t)$ is directly proportional to the stimulus at that time or some delayed version of it. This can be expressed as:\n",
    "     $$\n",
    "     r(t) = k \\cdot s(t - \\tau)\n",
    "     $$\n",
    "   - Here, $k$ is a scaling factor, and $\\tau$ is a delay.\n",
    "\n",
    "2. **General Linear Model**\n",
    "   - In a more general form, the response is modeled as a weighted sum of past stimuli. This can be expressed as:\n",
    "     $$\n",
    "     r(t) = \\sum_{k} f(k) \\cdot s(t - k)\n",
    "     $$\n",
    "   - This is equivalent to a linear convolution of the stimulus with a filter $f$, represented in integral form as:\n",
    "     $$\n",
    "     r(t) = \\int_{-\\infty}^{\\infty} f(\\tau) \\cdot s(t - \\tau) \\, d\\tau\n",
    "     $$\n",
    "\n",
    "**Types of Filters**\n",
    "1. **Running Average Filter**\n",
    "   - If $r(t)$ is a running average over $n$ time steps:\n",
    "     $$\n",
    "     f(k) = \\frac{1}{n} \\text{ for } k \\text{ in the window }\n",
    "     $$\n",
    "   - This filter smooths the input, averaging over a window of $n$ time steps.\n",
    "\n",
    "2. **Leaky Integrator**\n",
    "   - For a leaky integrator, where the response depends on past stimuli with an exponentially decreasing weight:\n",
    "     $$\n",
    "     f(t) = \\alpha e^{-\\alpha t}\n",
    "     $$\n",
    "   - This model captures the idea of a memory that decays over time, where $\\alpha$ controls the rate of decay.\n",
    "\n",
    "**Spatial Receptive Fields**\n",
    "1. **2D Receptive Fields**\n",
    "   - When considering spatial stimuli (e.g., visual input), the neuron's response is determined by how well the spatial pattern matches the neuron's receptive field $f(x, y)$. This can be expressed as:\n",
    "     $$\n",
    "     r(x, y) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x', y') \\cdot s(x - x', y - y') \\, dx' \\, dy'\n",
    "     $$\n",
    "   - Here, $f(x', y')$ is the receptive field function, and $s(x - x', y - y')$ is the spatial stimulus.\n",
    "\n",
    "2. **Difference of Gaussians (DoG)**\n",
    "   - A common receptive field model is the difference of Gaussians, which detects local changes:\n",
    "     $$\n",
    "     f(x, y) = \\text{Gaussian}_{\\text{center}} - \\text{Gaussian}_{\\text{surround}}\n",
    "     $$\n",
    "   - This model highlights edges by subtracting a broader, shallower Gaussian (surround) from a narrower, sharper Gaussian (center).\n",
    "\n",
    "**Combining Temporal and Spatial Models**\n",
    "- For a complete model incorporating both spatial and temporal dimensions, the filter $f$ becomes a 3D function of space and time:\n",
    "  $$\n",
    "  r(x, y, t) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x', y', \\tau) \\cdot s(x - x', y - y', t - \\tau) \\, dx' \\, dy' \\, d\\tau\n",
    "  $$\n",
    "\n",
    "**Nonlinearities**\n",
    "- Linear filters alone may not capture all neural response characteristics. To address issues like negative firing rates or unbounded increases, a nonlinear function $g$ is applied to the filtered output:\n",
    "  $$\n",
    "  r(t) = g\\left(\\int_{-\\infty}^{\\infty} f(\\tau) \\cdot s(t - \\tau) \\, d\\tau\\right)\n",
    "  $$\n",
    "- Common nonlinearities include functions that ensure positive firing rates and bounded responses (e.g., saturation functions).\n",
    "\n",
    "**Next Steps**\n",
    "- The next section will focus on how to derive these model components from actual neural data, allowing for more accurate modeling of neural responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Neural Encoding: Feature Selection\n",
    "\n",
    "In this section, the focus is on constructing a model to understand how neural systems process stimuli. Here's a summary of the key concepts discussed:\n",
    "\n",
    "### 1. **Modeling Neural Systems**\n",
    "\n",
    "- **Basic Model**: Consists of a linear filter (or feature) that extracts components from a stimulus and a nonlinear input-output function that maps the filtered stimulus onto the firing rate.\n",
    "  \n",
    "### 2. **Dimensionality Challenge**\n",
    "\n",
    "- **High-Dimensional Stimuli**: Stimuli like movies or images are high-dimensional (e.g., 300,000 values for a 1-megapixel image over time).\n",
    "- **Dimensionality Reduction**: It is impractical to sample the entire high-dimensional space, so we need to identify a small number of relevant components (features) that drive the neuron's response.\n",
    "\n",
    "### 3. **Finding Relevant Features**\n",
    "\n",
    "- **Discretization**: Represent the stimulus at different time points as components in a high-dimensional space.\n",
    "- **Gaussian White Noise**: Used as a stimulus to sample a broad spectrum of frequencies. The white noise is plotted in a high-dimensional space, with the goal of finding features that correlate with neural responses.\n",
    "\n",
    "### 4. **Spike-Triggered Average**\n",
    "\n",
    "- **Concept**: A method to find features that trigger neuronal spikes. By analyzing the stimulus segments that precede spikes and averaging them, we can identify features that are likely responsible for the neuronal firing.\n",
    "  \n",
    "### 5. **Dimensionality Reduction Techniques**\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: A method to identify the principal components that explain the most variance in the data. PCA helps to find low-dimensional structures in high-dimensional stimuli.\n",
    "- **Applications**: PCA is useful in various contexts, including sorting spike waveforms from different neurons and reconstructing images (e.g., eigenfaces).\n",
    "\n",
    "### 6. **Multiple Features**\n",
    "\n",
    "- **Combining Features**: Neurons may respond to multiple features, not just one. For example, an auditory neuron might respond to multiple frequencies, and the nonlinear response function combines these features in complex ways.\n",
    "\n",
    "### 7. **Example Applications**\n",
    "\n",
    "- **Retinal Ganglion Cells**: Demonstrated how PCA can reveal features like \"on\" and \"off\" responses in retinal ganglion cells, where the average stimulus might not reveal the underlying structure unless dimensionality reduction is applied.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The key takeaway is that identifying relevant features in high-dimensional stimuli involves dimensionality reduction techniques, such as spike-triggered averaging and PCA, to simplify the model and capture essential characteristics of the stimulus that drive neuronal responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Neural Encoding: Variability\n",
    "\n",
    "In this section, the discussion revolves around refining models of neural spike trains to better match observed data. Hereâ€™s a summary of the key points covered:\n",
    "\n",
    "1. **Stimulus and Model Assumptions**:\n",
    "   - **White Noise Stimulus**: White noise was previously used because it has no specific structure, making it easier to filter and analyze. However, real stimuli (like natural movies) have more complex structures that can influence the resulting models.\n",
    "   - **Gaussian Functions**: Gaussian functions are fundamental in modeling, with parameters like the mean (center) and variance (spread). The Gaussian distribution is useful because it simplifies the analysis of stimuli and their effects on neural responses.\n",
    "\n",
    "2. **Evaluating Filters with Information Theory**:\n",
    "   - **Kullback-Leibler Divergence (DKL)**: This measure evaluates the difference between two probability distributions. The goal is to find a filter \\( f \\) that maximizes the divergence between the prior and the spike-conditional distributions. This approach can generalize to complex stimuli beyond Gaussian noise.\n",
    "   - **Mutual Information**: The optimization of the filter can be interpreted as maximizing the mutual information between the spike train and the stimulus. This helps in identifying stimulus features that provide the most information about spike generation.\n",
    "\n",
    "3. **Poisson Process Model**:\n",
    "   - **Binomial to Poisson**: The Poisson distribution is a limit of the binomial distribution when the number of time bins becomes very large, and the probability of a spike in each bin becomes very small. This is useful for modeling spike trains with varying rates.\n",
    "   - **Properties**: The mean and variance of a Poisson distribution are equal, and this is tested using the Fano factor. If the Fano factor is close to 1, the spike train follows a Poisson process.\n",
    "   - **Exponential Intervals**: The time intervals between successive spikes in a Poisson process are exponentially distributed, which can be used to validate the model.\n",
    "\n",
    "4. **Model Extensions**:\n",
    "   - **Refractory Period**: Neurons cannot fire indefinitely; there are biophysical limits that prevent immediate firing after an action potential. These refractory periods can be incorporated into models using a post-spike filter that accounts for these limitations.\n",
    "   - **Generalized Linear Models (GLMs)**: GLMs extend the basic models by incorporating more complex factors, such as refractory effects and interactions with other neurons. The exponential nonlinearity is used to make optimization more tractable.\n",
    "\n",
    "5. **Time-Rescaling Theorem**:\n",
    "   - This theorem is used to test whether a model has captured all influences on spiking. By scaling the intervals between spikes by the predicted firing rate, one can check if the scaled intervals follow a Poisson distribution, indicating a well-fit model.\n",
    "\n",
    "6. **Future Directions**:\n",
    "   - The models discussed are powerful but may oversimplify neural coding by not accounting for all influences on neural responses. The field is evolving, and more general models may provide a richer understanding of neural processing.\n",
    "\n",
    "Overall, this summary captures the approach to refining neural models to better fit spike train data and the considerations for incorporating complex stimuli and intrinsic neural processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
