{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Synaptic Plasticity, Hebb's Rule, and Statistical Learning\n",
    "\n",
    "#### **Hebbian Learning**\n",
    "\n",
    "**Key Idea**: *\"Cells that fire together, wire together.\"* This principle suggests that the synaptic strength between two neurons increases when they are activated simultaneously.\n",
    "\n",
    "- **Hebb's Rule**: If neuron A repeatedly and persistently excites neuron B, the synapse between them strengthens.\n",
    "- **Mathematical Formulation**: \n",
    "  $$\n",
    "  \\Delta w_{ij} = \\eta \\cdot x_i \\cdot x_j\n",
    "  $$\n",
    "  where $ w_{ij} $ is the synaptic weight between neurons $ i $ and $ j $, $ \\eta $ is the learning rate, and $ x_i $ and $ x_j $ are the activities of the neurons.\n",
    "  \n",
    "- **Applications**: \n",
    "  - Unsupervised learning\n",
    "  - Development of associative memories\n",
    "  - Principal Component Analysis (PCA) via unsupervised learning\n",
    "  \n",
    "#### **Synaptic Plasticity**\n",
    "\n",
    "Synaptic plasticity refers to the ability of synapses to strengthen or weaken over time, based on the activity levels of the neurons.\n",
    "\n",
    "**Key Types**:\n",
    "1. **Long-Term Potentiation (LTP)**: A persistent strengthening of synapses based on recent patterns of activity. It is often regarded as a major cellular mechanism behind learning and memory.\n",
    "   - *Induced by*: High-frequency stimulation\n",
    "   - *Outcome*: Increases synaptic efficacy\n",
    "   \n",
    "2. **Long-Term Depression (LTD)**: The long-lasting decrease in synaptic strength, often occurring as a result of low-frequency stimulation.\n",
    "   - *Induced by*: Prolonged, low-frequency stimulation\n",
    "   - *Outcome*: Decreases synaptic efficacy\n",
    "\n",
    "**Mechanisms**:\n",
    "- **Spike-Timing Dependent Plasticity (STDP)**: \n",
    "  - If a presynaptic neuron fires before a postsynaptic neuron (in a short time window), synaptic strength increases (LTP).\n",
    "  - If the postsynaptic neuron fires before the presynaptic neuron, synaptic strength decreases (LTD).\n",
    "\n",
    "**Biological Mechanisms**:\n",
    "- **NMDA Receptors**: Key for LTP induction; they allow calcium influx when both presynaptic glutamate release and postsynaptic depolarization occur.\n",
    "- **AMPA Receptors**: LTP results in more AMPA receptors being inserted into the postsynaptic membrane, increasing synaptic strength.\n",
    "\n",
    "#### **Mathematical Models of Synaptic Plasticity**\n",
    "\n",
    "1. **Oja's Rule**:\n",
    "   - A modification of Hebbian learning that introduces weight normalization to prevent weights from growing indefinitely.\n",
    "   - **Formula**:\n",
    "     $$\n",
    "     \\Delta w_i = \\eta \\cdot (y \\cdot x_i - y^2 \\cdot w_i)\n",
    "     $$\n",
    "     where $ y = \\sum w_i x_i $.\n",
    "\n",
    "2. **BCM Model** (Bienenstock, Cooper, Munro):\n",
    "   - Accounts for both LTP and LTD and introduces a sliding threshold to regulate synaptic changes.\n",
    "   - The threshold depends on the average postsynaptic activity.\n",
    "\n",
    "#### **Principal Component Analysis (PCA)**\n",
    "\n",
    "**Key Idea**: PCA reduces the dimensionality of data by projecting it onto directions that capture the most variance.\n",
    "\n",
    "- **Relation to Hebbian Learning**:\n",
    "  - Hebbian learning can be used for unsupervised learning of the principal components of the input data.\n",
    "  - **Sangerâ€™s Rule**: A version of Hebbian learning that finds the principal components in a sequential manner.\n",
    "  \n",
    "- **Process**:\n",
    "  1. Center the data by subtracting the mean.\n",
    "  2. Compute covariance matrix.\n",
    "  3. Extract eigenvectors (principal components) and eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "### **Short Notes for Quick Review**\n",
    "\n",
    "- **Hebbian Learning**: Synapses strengthen when pre- and postsynaptic neurons fire together.\n",
    "- **LTP**: Synaptic strengthening due to high-frequency stimulation.\n",
    "- **LTD**: Synaptic weakening due to low-frequency stimulation.\n",
    "- **STDP**: Timing-dependent synaptic plasticity; pre-before-post (LTP), post-before-pre (LTD).\n",
    "- **NMDA receptors**: Crucial for LTP induction, require both presynaptic activity and postsynaptic depolarization.\n",
    "- **Oja's Rule**: Prevents weight explosion in Hebbian learning by introducing weight normalization.\n",
    "- **BCM Model**: Balances LTP and LTD with a sliding threshold.\n",
    "- **PCA**: Projects data onto axes that capture the most variance; related to Hebbian learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Introduction to Unsupervised Learning\n",
    "\n",
    "1. **Competitive Learning:**\n",
    "   - In a competitive learning neural network, each neuron is associated with a weight vector, and given an input, the neuron whose weight vector is closest to the input (i.e., the one with the highest activity) is declared the \"winner.\"\n",
    "   - The winner's weight vector is updated to move closer to the input. This is similar to a form of clustering, where neurons self-organize around input data clusters.\n",
    "   - Over time, neurons specialize in different regions of the input space, and the network partitions the data into clusters.\n",
    "\n",
    "2. **Self-Organizing Maps (SOM):**\n",
    "   - Similar to competitive learning, but with an important difference: not only the winner's weights are updated, but also the weights of neighboring neurons (those located close to the winner on a 2D grid).\n",
    "   - The aim is to preserve the topological relationships of the input data, mapping a potentially high-dimensional dataset to a lower-dimensional grid while maintaining the relative structure of the data.\n",
    "\n",
    "3. **Unsupervised Learning and Generative Models:**\n",
    "   - Unsupervised learning assumes that input data is generated by hidden causes, and the goal is to infer these causes (often through a probabilistic model).\n",
    "   - In the case of clustering, you can model the data as being generated by a mixture of Gaussians, with each cluster represented by a Gaussian distribution.\n",
    "   - The task is to learn the parameters of the Gaussians (means, variances, and priors) that best describe the data.\n",
    "\n",
    "4. **Expectation-Maximization (EM) Algorithm:**\n",
    "   - EM is a batch learning algorithm used to estimate the parameters of generative models.\n",
    "   - It alternates between two steps:\n",
    "     - **E-step:** Calculate the posterior probability that each data point belongs to each cluster (soft assignment).\n",
    "     - **M-step:** Update the parameters (mean, variance, and prior) of each Gaussian based on the soft assignments from the E-step.\n",
    "   - Over multiple iterations, the estimates of the cluster parameters improve, allowing the model to better represent the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Sparse Coding and Predictive Coding\n",
    "\n",
    "#### Principal Component Analysis (PCA) and Eigenfaces\n",
    "\n",
    "- **PCA Overview**:\n",
    "  - PCA can be used to represent natural images through eigenvectors of the input covariance matrix.\n",
    "  - Eigenvectors of the covariance matrix are known as \"Eigenfaces\" when applied to face images.\n",
    "  - Any image can be represented as a linear combination of these eigenfaces.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - Using the top M eigenvectors (principal components) allows for significant dimensionality reduction.\n",
    "  - Example: An image of size $1000 \\times 1000$ pixels (1 million pixels) can be represented with only a few eigenfaces (e.g., 10), achieving compression.\n",
    "\n",
    "- **Limitations**:\n",
    "  - PCA (eigenvectors) is not effective for extracting local components (e.g., eyes, nose) or for detecting features like edges in natural scenes.\n",
    "\n",
    "#### Sparse Coding\n",
    "\n",
    "- **Linear Model**:\n",
    "  - Represent natural scenes as a linear combination of basis vectors or features.\n",
    "  - Basis vectors are not limited to eigenvectors; they can be more general features.\n",
    "\n",
    "- **Generative Model**:\n",
    "  - Defined by specifying a prior probability distribution for causes and a likelihood function.\n",
    "  - Likelihood function: Assumes Gaussian noise; leads to a quadratic error term in log-likelihood.\n",
    "\n",
    "- **Sparse Representation**:\n",
    "  - Assumes that only a few causes (basis vectors) are active at a time.\n",
    "  - Sparse distributions (super Gaussian) have a peak at zero and heavy tails.\n",
    "  - Examples: Exponential distribution, Cauchy distribution.\n",
    "\n",
    "- **Learning Basis Vectors (Matrix G)**:\n",
    "  - Use Bayesian approach: Maximize posterior probability $p(v | u)$.\n",
    "  - Function $F$ combines reconstruction error and sparseness constraint.\n",
    "  - Update basis vectors using gradient ascent, similar to the EM algorithm.\n",
    "\n",
    "- **Comparison with PCA**:\n",
    "  - Sparse coding provides a sparser representation than PCA.\n",
    "  - Basis vectors learned resemble receptor fields in the primary visual cortex.\n",
    "\n",
    "#### Predictive Coding Networks\n",
    "\n",
    "- **Concept**:\n",
    "  - Use feedback connections for predictions and feedforward connections for error signals.\n",
    "  - Includes recurrent weights for modeling time-varying inputs.\n",
    "\n",
    "- **Predictive Coding Model**:\n",
    "  - Explains feedforward and feedback connections in the visual cortex.\n",
    "  - Feedback conveys predictions; feedforward conveys the error between prediction and actual input.\n",
    "\n",
    "- **Applications**:\n",
    "  - Helps explain contextual effects, surround suppression, and other phenomena in the visual cortex.\n",
    "\n",
    "- **Further Study**:\n",
    "  - Supplementary materials and papers for more details on predictive coding networks and related concepts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
