{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8eed93135b4c7e",
   "metadata": {},
   "source": [
    "# 4.1 Information and Entropy\n",
    "\n",
    "This lecture on information theory in computational neuroscience provides a framework for evaluating how neural spike trains encode information about stimuli. The three main topics covered are:\n",
    "\n",
    "1. **Entropy and Information**: Information theory quantifies \"surprise\" or uncertainty. The information from a spike is proportional to the surprise of seeing that spike, calculated as $-\\log_2(p)$, where $p$ is the probability of a spike. Entropy measures the average information, representing the variability of a random variable and quantifies the number of yes/no questions required to specify the variable's state.\n",
    "\n",
    "2. **Computing Information in Neural Spike Trains**: Neural spike trains can be evaluated for their encoding capacity by measuring their entropy. Greater variability in spike trains (higher entropy) suggests a higher capacity for encoding stimuli. When the probability of a spike $p = 0.5$, entropy is maximized, providing the most encoding potential.\n",
    "\n",
    "3. **Information and Coding**: The mutual information between a stimulus and a response quantifies how much of the variability in the neural response encodes information about the stimulus. Mutual information is calculated as the total entropy minus the noise entropy (entropy attributable to errors). As noise increases, mutual information decreases, reducing the neural code's effectiveness.\n",
    "\n",
    "The lecture emphasizes that coding capacity in neural systems is strongly linked to the variability of responses and their relationship to the stimuli they encode. The concepts of entropy, mutual information, and noise entropy are central to evaluating the efficiency of neural coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbccf7782e2b73ec",
   "metadata": {},
   "source": [
    "# 4.2 Calculating Information in Spike Trains\n",
    "\n",
    "This lecture segment explores the concept of computing information in spike trains, particularly focusing on how to measure the information encoded by spike patterns and individual spikes.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Mutual Information**: The information in spike trains is computed as the difference between the total response entropy (reflecting all possible spike patterns) and the mean noise entropy (reflecting variability in the response to a repeated stimulus). Mutual information quantifies how much information the neural response carries about the stimulus.\n",
    "\n",
    "2. **Spike Patterns**:\n",
    "   - **Binary Words**: Spike trains are converted into binary sequences (0s and 1s) by segmenting the response into time bins of size ∆t. A spike in a time bin is represented as 1, and no spike as 0. These binary sequences are termed \"words,\" with a length determined by a parameter \\(T\\).\n",
    "   - **Word Distribution**: From multiple trials, a distribution of words is created, representing different spike patterns over time. The total entropy of this distribution reflects the variability of spike patterns in response to various stimuli.\n",
    "\n",
    "3. **Noise Entropy**:\n",
    "   - **Stimulus Repetition**: By repeatedly presenting the same stimulus, the variability (or noise) in the responses is captured. This noise entropy is typically narrower than the total entropy and reflects the variability in spike patterns even when the stimulus is identical.\n",
    "\n",
    "4. **Time-Averaging**: Instead of averaging over all possible stimuli, time is used as a stand-in for averaging over stimuli. This trick assumes ergodicity, meaning that different stimuli are sampled over time in a manner that reflects their probability distribution.\n",
    "\n",
    "5. **Experimental Example**:\n",
    "   - **LGN Study**: An experiment on the lateral geniculate nucleus (LGN) used this method to analyze the information content of spike trains. The researchers tested how varying the bin size (∆t) and the word length (T) affected the information. They found that increasing temporal resolution (i.e., decreasing ∆t) and word length increased the information, but only up to a certain point. Beyond a particular bin size (around 2 milliseconds in this study), the information content no longer increased, suggesting that finer temporal details did not provide additional information.\n",
    "\n",
    "6. **Challenges with Estimating Information**:\n",
    "   - **Finite Data**: Estimating information from experimental data is challenging due to finite sample sizes. For long word lengths, the number of possible word combinations increases, but the data may not include enough samples to estimate the distribution accurately. Extrapolation techniques can help estimate entropy at longer word lengths, but they are not perfect.\n",
    "\n",
    "7. **Information in Single Spikes**:\n",
    "   - **Single Spike Information**: Information can also be computed for individual spikes, without needing an explicit model of what stimulus feature triggered the spike. This is done by comparing the total entropy (without knowledge of the stimulus) and the noise entropy (given the stimulus). This method can be generalized to compute the information for any event, not just spikes.\n",
    "\n",
    "   - **Factors Influencing Information**:\n",
    "     - **Timing Precision**: The precision of spike timing affects the amount of information. If spike timing is highly precise, the neural response conveys more information. Conversely, if the response is blurred, the information content is reduced.\n",
    "     - **Firing Rate**: The mean firing rate also influences the information content. A low firing rate implies that spikes occur infrequently, making each spike highly informative. However, while the information per spike may be high, the overall information rate (information per unit time) may be low.\n",
    "\n",
    "8. **Place Cell Example**: A hypothetical example involving rat hippocampal neurons (place cells) illustrates how information is encoded in neural activity. Place cells fire when the rat is in a specific location, and the shape of the firing rate function \\(R(t)\\) influences the amount of information that the spike train carries about the rat's location.\n",
    "\n",
    "### Summary:\n",
    "This section covers the methods for quantifying the information content in spike trains, focusing on spike patterns and single spikes. The key idea is that the information is captured by the difference between the total entropy of the neural responses and the noise entropy. Variations in bin size, word length, and spike timing precision all influence the amount of information encoded by the neural system. The challenge lies in accurately estimating information from finite experimental data, and different approaches have been developed to address these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b9426996dbfbc",
   "metadata": {},
   "source": [
    "# 4.3 Coding Principles\n",
    "\n",
    "The section explores how information theory can provide insights into how the neural code is adapted to the structure of natural signals. Here's a brief breakdown:\n",
    "\n",
    "1. **Natural Input Properties**: Natural stimuli, such as images and sounds, exhibit two key properties:\n",
    "   - **Dynamic Range**: There are large variations in light levels and contrast (in vision) or amplitude (in sound).\n",
    "   - **Scale-Invariance**: Similar structures appear at different scales in natural images.\n",
    "\n",
    "2. **Neural Code Efficiency**: Given these properties, neural systems must encode information efficiently despite their limited range of responses. This involves:\n",
    "   - **Maximizing Output Entropy**: The goal is to use all available response symbols equally often, which is equivalent to maximizing mutual information.\n",
    "   - **Histogram Equalization**: The input-output function of neurons should reflect the distribution of natural inputs. For example, neurons in the fly’s visual system adjust their response curve to match the cumulative distribution of natural contrasts.\n",
    "\n",
    "3. **Adaptation to Stimulus Statistics**: Sensory systems adjust not just over evolutionary timescales but also dynamically in response to changing stimulus statistics. For example:\n",
    "   - **Contrast Adaptation**: In the fly visual system, the response curve of neurons adapts to the local contrast distribution, adjusting the input-output curve to match the current stimulus.\n",
    "   - **Scaling with Standard Deviation**: Neurons adjust their response curves in real-time by normalizing the stimulus by its standard deviation, which allows them to encode stimuli across different dynamic ranges.\n",
    "\n",
    "4. **Feature Adaptation**: Beyond simple input-output curves, the neural code can adapt its features in response to changes in stimulus statistics. For example, the retina changes its spatial filtering properties depending on the light level.\n",
    "\n",
    "5. **Sparse Coding**: A significant principle in neural coding is **sparse coding**, where neurons are organized to minimize the number of active neurons for representing a stimulus. This principle suggests that the brain uses a basis set (localized oriented features) that excites a minimal number of neurons, leading to efficient coding.\n",
    "\n",
    "6. **Redundancy Reduction**: Initially, it was thought that neurons should code independently to maximize entropy, known as **redundancy reduction**. However, more recent work has shown that correlations between neurons can be advantageous for robustness and discrimination, leading to reconsideration of this idea.\n",
    "\n",
    "7. **Neural Coding Optimization**: Concepts such as maximizing entropy, redundancy reduction, and sparse coding have been used to explain the properties of receptive fields in different sensory systems, especially in the visual cortex.\n",
    "\n",
    "These information-theoretic ideas suggest that the neural code is shaped by both the statistics of natural inputs and the need for efficient and adaptable representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
